{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/Depth-Anything-jupyter/blob/main/Depth_Anything_Video_jupyter.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.dev/camenduru/Depth-Anything-Video-hf\n",
        "%cd /content/Depth-Anything-Video-hf\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/controlnet_demo1.png -d /content/Depth-Anything-Video-hf/assets -o controlnet_demo1.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/controlnet_demo2.png -d /content/Depth-Anything-Video-hf/assets -o controlnet_demo2.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples_video/davis_dolphins.mp4 -d /content/Depth-Anything-Video-hf/assets/examples_video -o davis_dolphins.mp4\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples_video/davis_rollercoaster.mp4 -d /content/Depth-Anything-Video-hf/assets/examples_video -o davis_rollercoaster.mp4\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples_video/davis_seasnake.mp4 -d /content/Depth-Anything-Video-hf/assets/examples_video -o davis_seasnake.mp4\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo1.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo1.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo10.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo10.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo11.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo11.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo12.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo12.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo13.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo13.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo14.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo14.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo15.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo15.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo16.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo16.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo17.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo17.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo18.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo18.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo19.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo19.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo2.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo2.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo20.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo20.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo3.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo3.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo4.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo4.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo5.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo5.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo7.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo7.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo8.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo8.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/examples/demo9.png -d /content/Depth-Anything-Video-hf/assets/examples -o demo9.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/paper.pdf -d /content/Depth-Anything-Video-hf/assets -o paper.pdf\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/teaser.png -d /content/Depth-Anything-Video-hf/assets -o teaser.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/video_edit/demo1_midas.mp4 -d /content/Depth-Anything-Video-hf/assets/video_edit -o demo1_midas.mp4\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/video_edit/demo1_ours.mp4 -d /content/Depth-Anything-Video-hf/assets/video_edit -o demo1_ours.mp4\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/video_edit/demo1_video.mp4 -d /content/Depth-Anything-Video-hf/assets/video_edit -o demo1_video.mp4\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/video_edit/demo2_midas.mp4 -d /content/Depth-Anything-Video-hf/assets/video_edit -o demo2_midas.mp4\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/video_edit/demo2_ours.mp4 -d /content/Depth-Anything-Video-hf/assets/video_edit -o demo2_ours.mp4\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/assets/video_edit/demo2_video.mp4 -d /content/Depth-Anything-Video-hf/assets/video_edit -o demo2_video.mp4\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/checkpoints/depth_anything_vitb14.pth -d /content/Depth-Anything-Video-hf/checkpoints -o depth_anything_vitb14.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/checkpoints/depth_anything_vitl14.pth -d /content/Depth-Anything-Video-hf/checkpoints -o depth_anything_vitl14.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/JohanDL/Depth-Anything-Video/resolve/main/checkpoints/depth_anything_vits14.pth -d /content/Depth-Anything-Video-hf/checkpoints -o depth_anything_vits14.pth\n",
        "\n",
        "!pip install -q gradio==4.16.0 gradio_imageslider\n",
        "\n",
        "import gradio as gr\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Compose\n",
        "import tempfile\n",
        "\n",
        "from depth_anything.dpt import DepthAnything\n",
        "from depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_depth(model, image):\n",
        "    return model(image)[\"depth\"]\n",
        "\n",
        "def make_video(video_path, outdir='./vis_video_depth',encoder='vitl'):\n",
        "    if encoder not in [\"vitl\",\"vitb\",\"vits\"]:\n",
        "        encoder = \"vits\"\n",
        "\n",
        "    mapper = {\"vits\":\"small\",\"vitb\":\"base\",\"vitl\":\"large\"}\n",
        "    # DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    # model = DepthAnything.from_pretrained('LiheYoung/depth_anything_vitl14').to(DEVICE).eval()\n",
        "    # Define path for temporary processed frames\n",
        "    temp_frame_dir = tempfile.mkdtemp()\n",
        "    \n",
        "    margin_width = 50\n",
        "    to_tensor_transform = transforms.ToTensor()\n",
        "\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    DEVICE = \"cuda\"\n",
        "    # depth_anything = DepthAnything.from_pretrained('LiheYoung/depth_anything_{}14'.format(encoder)).to(DEVICE).eval()\n",
        "    depth_anything = pipeline(task = \"depth-estimation\", model=f\"nielsr/depth-anything-{mapper[encoder]}\", device=0)\n",
        "    \n",
        "    # total_params = sum(param.numel() for param in depth_anything.parameters())\n",
        "    # print('Total parameters: {:.2f}M'.format(total_params / 1e6))\n",
        "    \n",
        "    transform = Compose([\n",
        "        Resize(\n",
        "            width=518,\n",
        "            height=518,\n",
        "            resize_target=False,\n",
        "            keep_aspect_ratio=True,\n",
        "            ensure_multiple_of=14,\n",
        "            resize_method='lower_bound',\n",
        "            image_interpolation_method=cv2.INTER_CUBIC,\n",
        "        ),\n",
        "        NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        PrepareForNet(),\n",
        "    ])\n",
        "\n",
        "    if os.path.isfile(video_path):\n",
        "        if video_path.endswith('txt'):\n",
        "            with open(video_path, 'r') as f:\n",
        "                lines = f.read().splitlines()\n",
        "        else:\n",
        "            filenames = [video_path]\n",
        "    else:\n",
        "        filenames = os.listdir(video_path)\n",
        "        filenames = [os.path.join(video_path, filename) for filename in filenames if not filename.startswith('.')]\n",
        "        filenames.sort()\n",
        "    \n",
        "    # os.makedirs(outdir, exist_ok=True)\n",
        "    \n",
        "    for k, filename in enumerate(filenames):\n",
        "        print('Progress {:}/{:},'.format(k+1, len(filenames)), 'Processing', filename)\n",
        "        \n",
        "        raw_video = cv2.VideoCapture(filename)\n",
        "        frame_width, frame_height = int(raw_video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(raw_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        frame_rate = int(raw_video.get(cv2.CAP_PROP_FPS))\n",
        "        output_width = frame_width * 2 + margin_width\n",
        "        \n",
        "        filename = os.path.basename(filename)\n",
        "        # output_path = os.path.join(outdir, filename[:filename.rfind('.')] + '_video_depth.mp4')\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmpfile:\n",
        "            output_path = tmpfile.name\n",
        "        #out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"avc1\"), frame_rate, (output_width, frame_height))\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_path, fourcc, frame_rate, (output_width, frame_height))\n",
        "        # count=0\n",
        "        while raw_video.isOpened():\n",
        "            ret, raw_frame = raw_video.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            \n",
        "            frame = cv2.cvtColor(raw_frame, cv2.COLOR_BGR2RGB) / 255.0\n",
        "            frame_pil =  Image.fromarray((frame * 255).astype(np.uint8))\n",
        "            frame = transform({'image': frame})['image']\n",
        "            \n",
        "            frame = torch.from_numpy(frame).unsqueeze(0).to(DEVICE)\n",
        "            \n",
        "            \n",
        "            depth = to_tensor_transform(predict_depth(depth_anything, frame_pil))\n",
        "\n",
        "            depth = F.interpolate(depth[None], (frame_height, frame_width), mode='bilinear', align_corners=False)[0, 0]\n",
        "            depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
        "            \n",
        "            depth = depth.cpu().numpy().astype(np.uint8)\n",
        "            depth_color = cv2.applyColorMap(depth, cv2.COLORMAP_INFERNO)\n",
        "            \n",
        "            split_region = np.ones((frame_height, margin_width, 3), dtype=np.uint8) * 255\n",
        "            combined_frame = cv2.hconcat([raw_frame, split_region, depth_color])\n",
        "            \n",
        "            # out.write(combined_frame)\n",
        "            # frame_path = os.path.join(temp_frame_dir, f\"frame_{count:05d}.png\")\n",
        "            # cv2.imwrite(frame_path, combined_frame)\n",
        "            out.write(combined_frame)\n",
        "            # count += 1\n",
        "        \n",
        "        raw_video.release()\n",
        "        out.release()\n",
        "        return output_path\n",
        "\n",
        "css = \"\"\"\n",
        "#img-display-container {\n",
        "    max-height: 100vh;\n",
        "    }\n",
        "#img-display-input {\n",
        "    max-height: 80vh;\n",
        "    }\n",
        "#img-display-output {\n",
        "    max-height: 80vh;\n",
        "    }\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "title = \"# Depth Anything Video Demo\"\n",
        "description = \"\"\"Depth Anything on full video files.\n",
        "\n",
        "Please refer to our [paper](https://arxiv.org/abs/2401.10891), [project page](https://depth-anything.github.io), or [github](https://github.com/LiheYoung/Depth-Anything) for more details.\"\"\"\n",
        "\n",
        "transform = Compose([\n",
        "        Resize(\n",
        "            width=518,\n",
        "            height=518,\n",
        "            resize_target=False,\n",
        "            keep_aspect_ratio=True,\n",
        "            ensure_multiple_of=14,\n",
        "            resize_method='lower_bound',\n",
        "            image_interpolation_method=cv2.INTER_CUBIC,\n",
        "        ),\n",
        "        NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        PrepareForNet(),\n",
        "])\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def predict_depth(model, image):\n",
        "#     return model(image)\n",
        "\n",
        "with gr.Blocks(css=css) as demo:\n",
        "    gr.Markdown(title)\n",
        "    gr.Markdown(description)\n",
        "    gr.Markdown(\"### Video Depth Prediction demo\")\n",
        "\n",
        "    with gr.Row():\n",
        "        input_video = gr.Video(label=\"Input Video\")\n",
        "        model_type = gr.Dropdown([\"vits\", \"vitb\", \"vitl\"], type=\"value\", label='Model Type')\n",
        "    submit = gr.Button(\"Submit\")\n",
        "    processed_video = gr.Video(label=\"Processed Video\")\n",
        "\n",
        "    def on_submit(uploaded_video,model_type):\n",
        "                \n",
        "        # Process the video and get the path of the output video\n",
        "        output_video_path = make_video(uploaded_video,encoder=model_type)\n",
        "\n",
        "        return output_video_path\n",
        "\n",
        "    submit.click(on_submit, inputs=[input_video, model_type], outputs=processed_video)\n",
        "\n",
        "    example_files = os.listdir('assets/examples_video')\n",
        "    example_files.sort()\n",
        "    example_files = [os.path.join('assets/examples_video', filename) for filename in example_files]\n",
        "    examples = gr.Examples(examples=example_files, inputs=[input_video], outputs=processed_video, fn=on_submit, cache_examples=False)\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    demo.queue().launch(share=True, debug=True, Inline=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
